{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9T_jM9lAWWFd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\program files\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import six\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from types import MethodType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgTP6HyHWf4S"
   },
   "outputs": [],
   "source": [
    "IMAGE_ORDERING = 'channels_last'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ci-EMPuoXh9V"
   },
   "outputs": [],
   "source": [
    "# models/model_utils.py\n",
    "def get_segmentation_model( input , output ):\n",
    "\n",
    "\timg_input = input\n",
    "\to = output\n",
    "\n",
    "\to_shape = Model(img_input , o ).output_shape\n",
    "\ti_shape = Model(img_input , o ).input_shape\n",
    "\n",
    "\tif IMAGE_ORDERING == 'channels_first':\n",
    "\t\toutput_height = o_shape[2]\n",
    "\t\toutput_width = o_shape[3]\n",
    "\t\tinput_height = i_shape[2]\n",
    "\t\tinput_width = i_shape[3]\n",
    "\t\tn_classes = o_shape[1]\n",
    "\t\to = (Reshape((  -1  , output_height*output_width   )))(o)\n",
    "\t\to = (Permute((2, 1)))(o)\n",
    "\telif IMAGE_ORDERING == 'channels_last':\n",
    "\t\toutput_height = o_shape[1]\n",
    "\t\toutput_width = o_shape[2]\n",
    "\t\tinput_height = i_shape[1]\n",
    "\t\tinput_width = i_shape[2]\n",
    "\t\tn_classes = o_shape[3]\n",
    "\t\to = (Reshape((   output_height*output_width , -1    )))(o)\n",
    "\n",
    "\to = (Activation('softmax'))(o)\n",
    "\tmodel = Model( img_input , o )\n",
    "\tmodel.output_width = output_width\n",
    "\tmodel.output_height = output_height\n",
    "\tmodel.n_classes = n_classes\n",
    "\tmodel.input_height = input_height\n",
    "\tmodel.input_width = input_width\n",
    "\tmodel.model_name = \"\"\n",
    "\n",
    "\tmodel.train = MethodType( train , model )\n",
    "\tmodel.predict_segmentation = MethodType( predict , model )\n",
    "\t#model.predict_multiple = MethodType( predict_multiple , model )\n",
    "\t#model.evaluate_segmentation = MethodType( evaluate , model )\n",
    "\n",
    "\n",
    "\treturn model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIYDTjRrbBrA"
   },
   "outputs": [],
   "source": [
    "def image_segmentation_generator( images_path , segs_path ,  batch_size,  n_classes , input_height , input_width , output_height , output_width  , do_augment=False ):\n",
    "\t\n",
    "\n",
    "\timg_seg_pairs = get_pairs_from_paths( images_path , segs_path )\n",
    "\trandom.shuffle( img_seg_pairs )\n",
    "\tzipped = itertools.cycle( img_seg_pairs  )\n",
    "\n",
    "\twhile True:\n",
    "\t\tX = []\n",
    "\t\tY = []\n",
    "\t\tfor _ in range( batch_size) :\n",
    "\t\t\tim , seg = next(zipped) \n",
    "\n",
    "\t\t\tim = cv2.imread(im , 1 )\n",
    "\t\t\tseg = cv2.imread(seg , 1 )\n",
    "\n",
    "\t\t\tif do_augment:\n",
    "\t\t\t\timg , seg[:,:,0] = augment_seg( img , seg[:,:,0] )\n",
    "\n",
    "\t\t\tX.append( get_image_arr(im , input_width , input_height ,odering=IMAGE_ORDERING )  )\n",
    "\t\t\tY.append( get_segmentation_arr( seg , n_classes , output_width , output_height )  )\n",
    "\n",
    "\t\tyield np.array(X) , np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rh9ty4XDZRlJ"
   },
   "outputs": [],
   "source": [
    "def train( model  , \n",
    "\t\ttrain_images  , \n",
    "\t\ttrain_annotations , \n",
    "\t\tinput_height=None , \n",
    "\t\tinput_width=None , \n",
    "\t\tn_classes=None,\n",
    "\t\tverify_dataset=True,\n",
    "\t\tcheckpoints_path=None , \n",
    "\t\tepochs = 5,\n",
    "\t\tbatch_size = 2,\n",
    "\t\tvalidate=False , \n",
    "\t\tval_images=None , \n",
    "\t\tval_annotations=None ,\n",
    "\t\tval_batch_size=2 , \n",
    "\t\tauto_resume_checkpoint=False ,\n",
    "\t\tload_weights=None ,\n",
    "\t\tsteps_per_epoch=512,\n",
    "\t\toptimizer_name='adadelta' \n",
    "\t):\n",
    "\n",
    "\n",
    "\tif  isinstance(model, six.string_types) : # check if user gives model name insteead of the model object\n",
    "\t\t# create the model from the name\n",
    "\t\tassert ( not n_classes is None ) , \"Please provide the n_classes\"\n",
    "\t\tif (not input_height is None ) and ( not input_width is None):\n",
    "\t\t\tmodel = model_from_name[ model ](  n_classes , input_height=input_height , input_width=input_width )\n",
    "\t\telse:\n",
    "\t\t\tmodel = model_from_name[ model ](  n_classes )\n",
    "\n",
    "\tn_classes = model.n_classes\n",
    "\tinput_height = model.input_height\n",
    "\tinput_width = model.input_width\n",
    "\toutput_height = model.output_height\n",
    "\toutput_width = model.output_width\n",
    "\n",
    "\n",
    "\tif validate:\n",
    "\t\tassert not (  val_images is None ) \n",
    "\t\tassert not (  val_annotations is None ) \n",
    "\n",
    "\tif not optimizer_name is None:\n",
    "\t\tmodel.compile(loss='categorical_crossentropy',\n",
    "\t\t\toptimizer= optimizer_name ,\n",
    "\t\t\tmetrics=['accuracy'])\n",
    "\n",
    "\tif not checkpoints_path is None:\n",
    "\t\topen( checkpoints_path+\"_config.json\" , \"w\" ).write( json.dumps( {\n",
    "\t\t\t\"model_class\" : model.model_name ,\n",
    "\t\t\t\"n_classes\" : n_classes ,\n",
    "\t\t\t\"input_height\" : input_height ,\n",
    "\t\t\t\"input_width\" : input_width ,\n",
    "\t\t\t\"output_height\" : output_height ,\n",
    "\t\t\t\"output_width\" : output_width \n",
    "\t\t}))\n",
    "\n",
    "\tif ( not (load_weights is None )) and  len( load_weights ) > 0:\n",
    "\t\tprint(\"Loading weights from \" , load_weights )\n",
    "\t\tmodel.load_weights(load_weights)\n",
    "\n",
    "\tif auto_resume_checkpoint and ( not checkpoints_path is None ):\n",
    "\t\tlatest_checkpoint = find_latest_checkpoint( checkpoints_path )\n",
    "\t\tif not latest_checkpoint is None:\n",
    "\t\t\tprint(\"Loading the weights from latest checkpoint \"  ,latest_checkpoint )\n",
    "\t\t\tmodel.load_weights( latest_checkpoint )\n",
    "\n",
    "\n",
    "\tif verify_dataset:\n",
    "\t\tprint(\"Verifying train dataset\")\n",
    "\t\tverify_segmentation_dataset( train_images , train_annotations , n_classes )\n",
    "\t\tif validate:\n",
    "\t\t\tprint(\"Verifying val dataset\")\n",
    "\t\t\tverify_segmentation_dataset( val_images , val_annotations , n_classes )\n",
    "\n",
    "\n",
    "\ttrain_gen = image_segmentation_generator( train_images , train_annotations ,  batch_size,  n_classes , input_height , input_width , output_height , output_width   )\n",
    "\n",
    "\n",
    "\tif validate:\n",
    "\t\tval_gen  = image_segmentation_generator( val_images , val_annotations ,  val_batch_size,  n_classes , input_height , input_width , output_height , output_width   )\n",
    "\n",
    "\n",
    "\tif not validate:\n",
    "\t\tfor ep in range( epochs ):\n",
    "\t\t\tprint(\"Starting Epoch \" , ep )\n",
    "\t\t\tmodel.fit_generator( train_gen , steps_per_epoch  , epochs=1 )\n",
    "\t\t\tif not checkpoints_path is None:\n",
    "\t\t\t\tmodel.save_weights( checkpoints_path + \".\" + str( ep ) )\n",
    "\t\t\t\tprint(\"saved \" , checkpoints_path + \".model.\" + str( ep ) )\n",
    "\t\t\tprint(\"Finished Epoch\" , ep )\n",
    "\telse:\n",
    "\t\tfor ep in range( epochs ):\n",
    "\t\t\tprint(\"Starting Epoch \" , ep )\n",
    "\t\t\tmodel.fit_generator( train_gen , steps_per_epoch  , validation_data=val_gen , validation_steps=200 ,  epochs=1 )\n",
    "\t\t\tif not checkpoints_path is None:\n",
    "\t\t\t\tmodel.save_weights( checkpoints_path + \".\" + str( ep )  )\n",
    "\t\t\t\tprint(\"saved \" , checkpoints_path + \".model.\" + str( ep ) )\n",
    "\t\t\tprint(\"Finished Epoch\" , ep )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SiI5sU5ZdQP"
   },
   "outputs": [],
   "source": [
    "def predict( model=None , inp=None , out_fname=None , checkpoints_path=None    ):\n",
    "\n",
    "\tif model is None and ( not checkpoints_path is None ):\n",
    "\t\tmodel = model_from_checkpoint_path(checkpoints_path)\n",
    "\n",
    "\tassert ( not inp is None )\n",
    "\tassert( (type(inp) is np.ndarray ) or  isinstance( inp , six.string_types)  ) , \"Inupt should be the CV image or the input file name\"\n",
    "\t\n",
    "\tif isinstance( inp , six.string_types)  :\n",
    "\t\tinp = cv2.imread(inp )\n",
    "\n",
    "\tassert len(inp.shape) == 3 , \"Image should be h,w,3 \"\n",
    "\torininal_h = inp.shape[0]\n",
    "\torininal_w = inp.shape[1]\n",
    "\n",
    "\n",
    "\toutput_width = model.output_width\n",
    "\toutput_height  = model.output_height\n",
    "\tinput_width = model.input_width\n",
    "\tinput_height = model.input_height\n",
    "\tn_classes = model.n_classes\n",
    "\n",
    "\tx = get_image_arr( inp , input_width  , input_height , odering=IMAGE_ORDERING )\n",
    "\tpr = model.predict( np.array([x]) )[0]\n",
    "\tpr = pr.reshape(( output_height ,  output_width , n_classes ) ).argmax( axis=2 )\n",
    "\n",
    "\tseg_img = np.zeros( ( output_height , output_width , 3  ) )\n",
    "\tcolors = class_colors\n",
    "\n",
    "\tfor c in range(n_classes):\n",
    "\t\tseg_img[:,:,0] += ( (pr[:,: ] == c )*( colors[c][0] )).astype('uint8')\n",
    "\t\tseg_img[:,:,1] += ((pr[:,: ] == c )*( colors[c][1] )).astype('uint8')\n",
    "\t\tseg_img[:,:,2] += ((pr[:,: ] == c )*( colors[c][2] )).astype('uint8')\n",
    "\n",
    "\tseg_img = cv2.resize(seg_img  , (orininal_w , orininal_h ))\n",
    "\n",
    "\tif not out_fname is None:\n",
    "\t\tcv2.imwrite(  out_fname , seg_img )\n",
    "\n",
    "\n",
    "\treturn pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWA9DsArXx0i"
   },
   "outputs": [],
   "source": [
    "#models/vgg16.py\n",
    "if IMAGE_ORDERING == 'channels_first':\n",
    "\tpretrained_url = \"C:\\\\Users\\\\theza\\\\Documents\\\\Uni\\\\MIT\\\\2019\\\\TP\\\\Project\\\\Meal-Compliance-Project\\\\Image-segmentation\\\\models\\\\model_latest.h5\"\n",
    "elif IMAGE_ORDERING == 'channels_last':\n",
    "\tpretrained_url = \"C:\\\\Users\\\\theza\\\\Documents\\\\Uni\\\\MIT\\\\2019\\\\TP\\\\Project\\\\Meal-Compliance-Project\\\\Image-segmentation\\\\models\\\\model_latest.h5\"\n",
    "\n",
    "\n",
    "def get_vgg_encoder( input_height=224 ,  input_width=224 , pretrained='imagenet'):\n",
    "\n",
    "\tassert input_height%32 == 0\n",
    "\tassert input_width%32 == 0\n",
    "\n",
    "\tif IMAGE_ORDERING == 'channels_first':\n",
    "\t\timg_input = Input(shape=(3,input_height,input_width))\n",
    "\telif IMAGE_ORDERING == 'channels_last':\n",
    "\t\timg_input = Input(shape=(input_height,input_width , 3 ))\n",
    "\n",
    "\tx = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1', data_format=IMAGE_ORDERING )(img_input)\n",
    "\tx = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool', data_format=IMAGE_ORDERING )(x)\n",
    "\tf1 = x\n",
    "\t# Block 2\n",
    "\tx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format=IMAGE_ORDERING )(x)\n",
    "\tf2 = x\n",
    "\n",
    "\t# Block 3\n",
    "\tx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool', data_format=IMAGE_ORDERING )(x)\n",
    "\tf3 = x\n",
    "\n",
    "\t# Block 4\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool', data_format=IMAGE_ORDERING )(x)\n",
    "\tf4 = x\n",
    "\n",
    "\t# Block 5\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "\tx = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool', data_format=IMAGE_ORDERING )(x)\n",
    "\tf5 = x\n",
    "\n",
    "\t\n",
    "\tif pretrained == 'imagenet':\n",
    "\t\tVGG_Weights_path = keras.utils.get_file( pretrained_url.split(\"/\")[-1] , pretrained_url  )\n",
    "\t\tModel(  img_input , x  ).load_weights(VGG_Weights_path)\n",
    "\n",
    "\n",
    "\treturn img_input , [f1 , f2 , f3 , f4 , f5 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mo2NwIg2aHfE"
   },
   "outputs": [],
   "source": [
    "def verify_segmentation_dataset( images_path , segs_path , n_classes ):\n",
    "\t\n",
    "\timg_seg_pairs = get_pairs_from_paths( images_path , segs_path )\n",
    "\t\n",
    "\tassert len(img_seg_pairs)>0 , \"Dataset looks empty or path is wrong \"\n",
    "\t\n",
    "\tfor im_fn , seg_fn in tqdm(img_seg_pairs) :\n",
    "\t\timg = cv2.imread( im_fn )\n",
    "\t\tseg = cv2.imread( seg_fn )\n",
    "\n",
    "\t\tassert ( img.shape[0]==seg.shape[0] and img.shape[1]==seg.shape[1] ) , \"The size of image and the annotation does not match or they are corrupt \"+ im_fn + \" \" + seg_fn\n",
    "\t\tassert ( np.max(seg[:,:,0]) < n_classes) , \"The pixel values of seg image should be from 0 to \"+str(n_classes-1) + \" . Found pixel value \"+str(np.max(seg[:,:,0]))\n",
    "\n",
    "\tprint(\"Dataset verified! \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-5ZI1uoaN2W"
   },
   "outputs": [],
   "source": [
    "def get_pairs_from_paths( images_path , segs_path ):\n",
    "\timages = glob.glob( os.path.join(images_path,\"*.jpg\")  ) + glob.glob( os.path.join(images_path,\"*.png\")  ) +  glob.glob( os.path.join(images_path,\"*.jpeg\")  )\n",
    "\tsegmentations  =  glob.glob( os.path.join(segs_path,\"*.png\")  ) \n",
    "\n",
    "\tsegmentations_d = dict( zip(segmentations,segmentations ))\n",
    "\n",
    "\tret = []\n",
    "\n",
    "\tfor im in images:\n",
    "\t\tseg_bnme = os.path.basename(im).replace(\".jpg\" , \".png\").replace(\".jpeg\" , \".png\")\n",
    "\t\tseg = os.path.join( segs_path , seg_bnme  )\n",
    "\t\tassert ( seg in segmentations_d ),  (im + \" is present in \"+images_path +\" but \"+seg_bnme+\" is not found in \"+segs_path + \" . Make sure annotation image are in .png\"  )\n",
    "\t\tret.append((im , seg) )\n",
    "\n",
    "\treturn ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mUEjJI_hbTHu"
   },
   "outputs": [],
   "source": [
    "def get_image_arr( path , width , height , imgNorm=\"sub_mean\" , odering='channels_first' ):\n",
    "\n",
    "\n",
    "\tif type( path ) is np.ndarray:\n",
    "\t\timg = path\n",
    "\telse:\n",
    "\t\timg = cv2.imread(path, 1)\n",
    "\n",
    "\tif imgNorm == \"sub_and_divide\":\n",
    "\t\timg = np.float32(cv2.resize(img, ( width , height ))) / 127.5 - 1\n",
    "\telif imgNorm == \"sub_mean\":\n",
    "\t\timg = cv2.resize(img, ( width , height ))\n",
    "\t\timg = img.astype(np.float32)\n",
    "\t\timg[:,:,0] -= 103.939\n",
    "\t\timg[:,:,1] -= 116.779\n",
    "\t\timg[:,:,2] -= 123.68\n",
    "\t\timg = img[ : , : , ::-1 ]\n",
    "\telif imgNorm == \"divide\":\n",
    "\t\timg = cv2.resize(img, ( width , height ))\n",
    "\t\timg = img.astype(np.float32)\n",
    "\t\timg = img/255.0\n",
    "\n",
    "\tif odering == 'channels_first':\n",
    "\t\timg = np.rollaxis(img, 2, 0)\n",
    "\treturn img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nlZk35wbbFN"
   },
   "outputs": [],
   "source": [
    "def get_segmentation_arr( path , nClasses ,  width , height , no_reshape=False ):\n",
    "\n",
    "\tseg_labels = np.zeros((  height , width  , nClasses ))\n",
    "\t\t\n",
    "\tif type( path ) is np.ndarray:\n",
    "\t\timg = path\n",
    "\telse:\n",
    "\t\timg = cv2.imread(path, 1)\n",
    "\n",
    "\timg = cv2.resize(img, ( width , height ) , interpolation=cv2.INTER_NEAREST )\n",
    "\timg = img[:, : , 0]\n",
    "\n",
    "\tfor c in range(nClasses):\n",
    "\t\tseg_labels[: , : , c ] = (img == c ).astype(int)\n",
    "\n",
    "\n",
    "\t\n",
    "\tif no_reshape:\n",
    "\t\treturn seg_labels\n",
    "\n",
    "\tseg_labels = np.reshape(seg_labels, ( width*height , nClasses ))\n",
    "\treturn seg_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7nZOGV_PYjjX"
   },
   "outputs": [],
   "source": [
    "if IMAGE_ORDERING == 'channels_first':\n",
    "\tMERGE_AXIS = 1\n",
    "elif IMAGE_ORDERING == 'channels_last':\n",
    "\tMERGE_AXIS = -1\n",
    "\n",
    "\n",
    "\n",
    "def unet_mini( n_classes , input_height=360, input_width=480   ):\n",
    "\n",
    "\tif IMAGE_ORDERING == 'channels_first':\n",
    "\t\timg_input = Input(shape=(3,input_height,input_width))\n",
    "\telif IMAGE_ORDERING == 'channels_last':\n",
    "\t\timg_input = Input(shape=(input_height,input_width , 3 ))\n",
    "\n",
    "\n",
    "\tconv1 = Conv2D(32, (3, 3), data_format=IMAGE_ORDERING, activation='relu', padding='same')(img_input)\n",
    "\tconv1 = Dropout(0.2)(conv1)\n",
    "\tconv1 = Conv2D(32, (3, 3), data_format=IMAGE_ORDERING, activation='relu', padding='same')(conv1)\n",
    "\tpool1 = MaxPooling2D((2, 2), data_format=IMAGE_ORDERING)(conv1)\n",
    "\t\n",
    "\tconv2 = Conv2D(64, (3, 3), data_format=IMAGE_ORDERING, activation='relu', padding='same')(pool1)\n",
    "\tconv2 = Dropout(0.2)(conv2)\n",
    "\tconv2 = Conv2D(64, (3, 3), data_format=IMAGE_ORDERING, activation='relu', padding='same')(conv2)\n",
    "\tpool2 = MaxPooling2D((2, 2), data_format=IMAGE_ORDERING)(conv2)\n",
    "\t\n",
    "\tconv3 = Conv2D(128, (3, 3), data_format=IMAGE_ORDERING, activation='relu', padding='same')(pool2)\n",
    "\tconv3 = Dropout(0.2)(conv3)\n",
    "\tconv3 = Conv2D(128, (3, 3), data_format=IMAGE_ORDERING, activation='relu', padding='same')(conv3)\n",
    "\n",
    "\tup1 = concatenate([UpSampling2D((2, 2), data_format=IMAGE_ORDERING)(conv3), conv2], axis=MERGE_AXIS)\n",
    "\tconv4 = Conv2D(64, (3, 3), data_format=IMAGE_ORDERING, activation='relu', padding='same')(up1)\n",
    "\tconv4 = Dropout(0.2)(conv4)\n",
    "\tconv4 = Conv2D(64, (3, 3), data_format=IMAGE_ORDERING, activation='relu', padding='same')(conv4)\n",
    "\t\n",
    "\tup2 = concatenate([UpSampling2D((2, 2), data_format=IMAGE_ORDERING)(conv4), conv1], axis=MERGE_AXIS)\n",
    "\tconv5 = Conv2D(32, (3, 3), data_format=IMAGE_ORDERING, activation='relu', padding='same')(up2)\n",
    "\tconv5 = Dropout(0.2)(conv5)\n",
    "\tconv5 = Conv2D(32, (3, 3), data_format=IMAGE_ORDERING, activation='relu', padding='same')(conv5)\n",
    "\t\n",
    "\to = Conv2D( n_classes, (1, 1) , data_format=IMAGE_ORDERING ,padding='same')(conv5)\n",
    "\n",
    "\tmodel = get_segmentation_model(img_input , o )\n",
    "\tmodel.model_name = \"unet_mini\"\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def _unet( n_classes , encoder , l1_skip_conn=True,  input_height=416, input_width=608  ):\n",
    "\n",
    "\timg_input , levels = encoder( input_height=input_height ,  input_width=input_width )\n",
    "\t[f1 , f2 , f3 , f4 , f5 ] = levels \n",
    "\n",
    "\to = f4\n",
    "\n",
    "\to = ( ZeroPadding2D( (1,1) , data_format=IMAGE_ORDERING ))(o)\n",
    "\to = ( Conv2D(512, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n",
    "\to = ( BatchNormalization())(o)\n",
    "\n",
    "\to = ( UpSampling2D( (2,2), data_format=IMAGE_ORDERING))(o)\n",
    "\to = ( concatenate([ o ,f3],axis=MERGE_AXIS )  )\n",
    "\to = ( ZeroPadding2D( (1,1), data_format=IMAGE_ORDERING))(o)\n",
    "\to = ( Conv2D( 256, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n",
    "\to = ( BatchNormalization())(o)\n",
    "\n",
    "\to = ( UpSampling2D( (2,2), data_format=IMAGE_ORDERING))(o)\n",
    "\to = ( concatenate([o,f2],axis=MERGE_AXIS ) )\n",
    "\to = ( ZeroPadding2D((1,1) , data_format=IMAGE_ORDERING ))(o)\n",
    "\to = ( Conv2D( 128 , (3, 3), padding='valid' , data_format=IMAGE_ORDERING ) )(o)\n",
    "\to = ( BatchNormalization())(o)\n",
    "\n",
    "\to = ( UpSampling2D( (2,2), data_format=IMAGE_ORDERING))(o)\n",
    "\t\n",
    "\tif l1_skip_conn:\n",
    "\t\to = ( concatenate([o,f1],axis=MERGE_AXIS ) )\n",
    "\n",
    "\to = ( ZeroPadding2D((1,1)  , data_format=IMAGE_ORDERING ))(o)\n",
    "\to = ( Conv2D( 64 , (3, 3), padding='valid'  , data_format=IMAGE_ORDERING ))(o)\n",
    "\to = ( BatchNormalization())(o)\n",
    "\n",
    "\to =  Conv2D( n_classes , (3, 3) , padding='same', data_format=IMAGE_ORDERING )( o )\n",
    "\t\n",
    "\tmodel = get_segmentation_model(img_input , o )\n",
    "\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def unet(  n_classes ,  input_height=416, input_width=608 , encoder_level=3 ) : \n",
    "\t\n",
    "\tmodel =  _unet( n_classes , vanilla_encoder ,  input_height=input_height, input_width=input_width  )\n",
    "\tmodel.model_name = \"unet\"\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def vgg_unet( n_classes ,  input_height=416, input_width=608 , encoder_level=3):\n",
    "\n",
    "\tmodel =  _unet( n_classes , get_vgg_encoder ,  input_height=input_height, input_width=input_width  )\n",
    "\tmodel.model_name = \"vgg_unet\"\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "hnsKd3gDYPq6",
    "outputId": "c8b5a84a-e8a4-4885-c8e9-64e78ae08d71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0916 15:22:35.063826 17932 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0916 15:22:35.092767 17932 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0916 15:22:35.096753 17932 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0916 15:22:35.143680 17932 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 19 layers into a model with 13 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-03913cfb097a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg_unet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m33\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[0minput_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1056\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-1a7a03b5e57b>\u001b[0m in \u001b[0;36mvgg_unet\u001b[1;34m(n_classes, input_height, input_width, encoder_level)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvgg_unet\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[0minput_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m608\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mencoder_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0m_unet\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mget_vgg_encoder\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[0minput_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_width\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"vgg_unet\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-1a7a03b5e57b>\u001b[0m in \u001b[0;36m_unet\u001b[1;34m(n_classes, encoder, l1_skip_conn, input_height, input_width)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_unet\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0ml1_skip_conn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0minput_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m608\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mimg_input\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0minput_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_height\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[0minput_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_width\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mf1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mf3\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mf4\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mf5\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-aea11d3ebdaa>\u001b[0m in \u001b[0;36mget_vgg_encoder\u001b[1;34m(input_height, input_width, pretrained)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpretrained\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mVGG_Weights_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_file\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mpretrained_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpretrained_url\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m  \u001b[0mimg_input\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVGG_Weights_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1164\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[1;32m-> 1166\u001b[1;33m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers, reshape)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                          \u001b[1;34m'containing '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                          \u001b[1;34m' layers into a model with '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to load a weight file containing 19 layers into a model with 13 layers."
     ]
    }
   ],
   "source": [
    "model = vgg_unet(n_classes=33 ,  input_height=1056, input_width=1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "Sale9JVrZqSX",
    "outputId": "60460e8c-7cac-4d58-fd82-3d5f66647a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "huU6Jg3mZwzv",
    "outputId": "47e6421f-077c-4824-f8dd-294edfafd361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/content/content/drive/My': No such file or directory\n",
      "ls: cannot access 'Drive/example_dataset/XTrain': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import os # File/directory operations\n",
    "rootPath = os.path.abspath(os.path.join('content','drive','My Drive','example_dataset','XTrain'))\n",
    "!ls $rootPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iVoOmdjQc8M7",
    "outputId": "58949587-8299-46c4-e61c-b8251c2bfd05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1a.JPG\t1x.JPG\t2r.JPG\t3n.JPG\t4j.JPG\t5c.JPG\t6b.JPG\t6w.JPG\t7q.JPG\t8l.JPG\n",
      "1b.JPG\t1z.JPG\t2u.JPG\t3p.JPG\t4k.JPG\t5h.JPG\t6c.JPG\t6z.JPG\t7r.JPG\t8m.JPG\n",
      "1c.JPG\t2b.JPG\t2v.JPG\t3q.JPG\t4l.JPG\t5i.JPG\t6d.JPG\t7b.JPG\t7s.JPG\t8n.JPG\n",
      "1d.JPG\t2c.JPG\t2w.JPG\t3r.JPG\t4n.JPG\t5j.JPG\t6h.JPG\t7d.JPG\t7t.JPG\t8p.JPG\n",
      "1g.JPG\t2d.JPG\t2x.JPG\t3s.JPG\t4p.JPG\t5k.JPG\t6j.JPG\t7e.JPG\t7v.JPG\t8q.JPG\n",
      "1j.JPG\t2f.JPG\t2y.JPG\t3u.JPG\t4q.JPG\t5l.JPG\t6k.JPG\t7f.JPG\t7w.JPG\t8r.JPG\n",
      "1k.JPG\t2g.JPG\t2z.JPG\t3v.JPG\t4r.JPG\t5n.JPG\t6m.JPG\t7g.JPG\t7x.JPG\t8s.JPG\n",
      "1l.JPG\t2i.JPG\t3b.JPG\t3z.JPG\t4t.JPG\t5p.JPG\t6n.JPG\t7h.JPG\t7y.JPG\t8t.JPG\n",
      "1n.JPG\t2j.JPG\t3c.JPG\t4a.JPG\t4u.JPG\t5q.JPG\t6o.JPG\t7i.JPG\t7z.JPG\t8u.JPG\n",
      "1o.JPG\t2k.JPG\t3f.JPG\t4b.JPG\t4v.JPG\t5r.JPG\t6q.JPG\t7j.JPG\t8a.JPG\t8v.JPG\n",
      "1r.JPG\t2m.JPG\t3g.JPG\t4d.JPG\t4w.JPG\t5t.JPG\t6r.JPG\t7l.JPG\t8b.JPG\t8x.JPG\n",
      "1t.JPG\t2n.JPG\t3h.JPG\t4e.JPG\t4y.JPG\t5x.JPG\t6s.JPG\t7m.JPG\t8d.JPG\t8y.JPG\n",
      "1u.JPG\t2p.JPG\t3k.JPG\t4g.JPG\t4z.JPG\t5y.JPG\t6u.JPG\t7n.JPG\t8g.JPG\t8z.JPG\n",
      "1v.JPG\t2q.JPG\t3m.JPG\t4h.JPG\t5b.JPG\t6a.JPG\t6v.JPG\t7p.JPG\t8k.JPG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/content/drive/My Drive/example_dataset/XTrain/1n.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1g.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1l.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1j.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1b.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1k.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1o.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1d.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1a.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1c.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1r.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1v.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1t.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2f.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2b.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1x.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1u.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/1z.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2g.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2i.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2j.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2k.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2m.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2n.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2p.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2q.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2r.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2u.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2v.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2w.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2x.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2y.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2z.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3b.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3c.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3f.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3g.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3h.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3k.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3m.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3n.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3p.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3q.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3r.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2c.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/2d.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3s.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4a.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3v.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3z.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4b.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4e.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4d.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4j.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/3u.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4h.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4g.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4k.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4l.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4n.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4u.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4t.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4v.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4q.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4r.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4p.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4z.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4w.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/4y.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5b.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5n.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5r.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5h.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5i.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5k.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5j.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5l.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5c.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5p.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5t.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5q.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5x.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/5y.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6n.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6j.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6h.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6k.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6a.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6b.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6d.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6c.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6m.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6o.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6u.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7e.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6r.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7b.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6v.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6z.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7d.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6q.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6s.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7f.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/6w.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7g.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7h.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7s.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7p.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7n.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7j.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7i.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7q.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7l.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7t.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7m.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7r.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7v.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7x.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8l.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8a.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7y.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7w.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8d.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/7z.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8b.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8g.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8k.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8m.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8n.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8p.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8q.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8v.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8t.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8r.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8z.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8s.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8u.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8y.JPG',\n",
       " '/content/drive/My Drive/example_dataset/XTrain/8x.JPG']"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ls '/content/drive/My Drive/example_dataset/XTrain'\n",
    "glob.glob( os.path.join(\"/content/drive/My Drive/example_dataset/XTrain\",\"*\")  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "colab_type": "code",
    "id": "Rx5EIa1yw8Yu",
    "outputId": "4d216b88-c18a-4ed5-cfc8-90d200fcc5fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying train dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▍         | 1/21 [00:00<00:18,  1.10it/s]\u001b[A\n",
      " 10%|▉         | 2/21 [00:01<00:16,  1.13it/s]\u001b[A\n",
      " 14%|█▍        | 3/21 [00:02<00:16,  1.08it/s]\u001b[A\n",
      " 19%|█▉        | 4/21 [00:03<00:16,  1.00it/s]\u001b[A\n",
      " 24%|██▍       | 5/21 [00:05<00:16,  1.04s/it]\u001b[A\n",
      " 29%|██▊       | 6/21 [00:05<00:14,  1.01it/s]\u001b[A\n",
      " 33%|███▎      | 7/21 [00:06<00:13,  1.03it/s]\u001b[A\n",
      " 38%|███▊      | 8/21 [00:07<00:12,  1.07it/s]\u001b[A\n",
      " 43%|████▎     | 9/21 [00:08<00:10,  1.09it/s]\u001b[A\n",
      " 48%|████▊     | 10/21 [00:09<00:10,  1.02it/s]\u001b[A\n",
      " 52%|█████▏    | 11/21 [00:10<00:09,  1.09it/s]\u001b[A\n",
      " 57%|█████▋    | 12/21 [00:11<00:08,  1.01it/s]\u001b[A\n",
      " 62%|██████▏   | 13/21 [00:12<00:07,  1.13it/s]\u001b[A\n",
      " 67%|██████▋   | 14/21 [00:13<00:05,  1.17it/s]\u001b[A\n",
      " 71%|███████▏  | 15/21 [00:14<00:05,  1.06it/s]\u001b[A\n",
      " 76%|███████▌  | 16/21 [00:15<00:04,  1.04it/s]\u001b[A\n",
      " 81%|████████  | 17/21 [00:16<00:03,  1.00it/s]\u001b[A\n",
      " 86%|████████▌ | 18/21 [00:17<00:02,  1.06it/s]\u001b[A\n",
      " 90%|█████████ | 19/21 [00:18<00:01,  1.07it/s]\u001b[A\n",
      " 95%|█████████▌| 20/21 [00:20<00:01,  1.27s/it]\u001b[A\n",
      "100%|██████████| 21/21 [00:20<00:00,  1.14s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset verified! \n",
      "Starting Epoch  0\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 877s 2s/step - loss: 0.1211 - acc: 0.9600\n",
      "Finished Epoch 0\n",
      "Starting Epoch  1\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 874s 2s/step - loss: 0.0787 - acc: 0.9782\n",
      "Finished Epoch 1\n",
      "Starting Epoch  2\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 872s 2s/step - loss: 0.0387 - acc: 0.9887\n",
      "Finished Epoch 2\n",
      "Starting Epoch  3\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 872s 2s/step - loss: 0.0341 - acc: 0.9911\n",
      "Finished Epoch 3\n",
      "Starting Epoch  4\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 872s 2s/step - loss: 0.0076 - acc: 0.9974\n",
      "Finished Epoch 4\n"
     ]
    }
   ],
   "source": [
    "model.train( \n",
    "    train_images =  \"/content/drive/My Drive/data/XTrain\",\n",
    "    train_annotations = \"/content/drive/My Drive/data/yTrain\",\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "YarRviQfm9_d",
    "outputId": "3a93271f-e3e3-4678-979c-9b837a8f511d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/drive/My Drive/data/XTest/1c.jpg', '/content/drive/My Drive/data/XTest/3c.jpg', '/content/drive/My Drive/data/XTest/3d.jpg', '/content/drive/My Drive/data/XTest/4b.jpg', '/content/drive/My Drive/data/XTest/4c.jpg', '/content/drive/My Drive/data/XTest/5a.jpg', '/content/drive/My Drive/data/XTest/6a.jpg', '/content/drive/My Drive/data/XTest/6c.jpg', '/content/drive/My Drive/data/XTest/7a.jpg', '/content/drive/My Drive/data/XTest/7c.jpg', '/content/drive/My Drive/data/XTest/8a.jpg']\n",
      "['/content/drive/My Drive/data/yTest/1c.png', '/content/drive/My Drive/data/yTest/3c.png', '/content/drive/My Drive/data/yTest/3d.png', '/content/drive/My Drive/data/yTest/4b.png', '/content/drive/My Drive/data/yTest/4c.png', '/content/drive/My Drive/data/yTest/5a.png', '/content/drive/My Drive/data/yTest/6a.png', '/content/drive/My Drive/data/yTest/6c.png', '/content/drive/My Drive/data/yTest/7a.png', '/content/drive/My Drive/data/yTest/7c.png', '/content/drive/My Drive/data/yTest/8a.png']\n"
     ]
    }
   ],
   "source": [
    "class_colors = [  ( random.randint(0,255),random.randint(0,255),random.randint(0,255)   ) for _ in range(5000)  ]\n",
    "test_images =  \"/content/drive/My Drive/data/XTest\"\n",
    "test_annotations = \"/content/drive/My Drive/data/yTest\"\n",
    "testIm = glob.glob( os.path.join(test_images,\"*\")  ) \n",
    "testAn = glob.glob( os.path.join(test_annotations,\"*\")  )\n",
    "\n",
    "train_images =  \"/content/drive/My Drive/data/XTrain\"\n",
    "train_annotations = \"/content/drive/My Drive/data/yTrain\"\n",
    "trainIm = glob.glob( os.path.join(train_images,\"*\")  ) \n",
    "trainAn = glob.glob( os.path.join(train_annotations,\"*\")  )\n",
    "\n",
    "\n",
    "testIm.sort()\n",
    "testAn.sort()\n",
    "trainIm.sort()\n",
    "trainAn.sort()\n",
    "print(testIm)\n",
    "print(testAn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRT1uxPOzUC9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from skimage.transform import resize # Resize images\n",
    "\n",
    "def res(annot, heightNew, width):\n",
    "  annotNew = resize(annot, (heightNew, width),mode='edge', anti_aliasing=False,\n",
    "                             anti_aliasing_sigma=None,preserve_range=True,\n",
    "                             order=0).astype(int)\n",
    "  df = (pd.DataFrame(annotNew))\n",
    "  _, b = pd.factorize(df.values.T.reshape(-1, ))  \n",
    "\n",
    "  # print(df.apply(lambda x: pd.Categorical(x, b).codes).values.shape)\n",
    "  annotNewOut = df.apply(lambda x: pd.Categorical(x, b).codes).values\n",
    "  return annotNewOut\n",
    "\n",
    "def evaluate( model=None , inp_images=None , annotations=None):\n",
    "  ious = []\n",
    "  for im, an in zip(inp_images,annotations):\n",
    "    img_true =  res(cv2.cvtColor(cv2.imread(an), cv2.COLOR_BGR2GRAY),528,800)\n",
    "    img_pred = predict(model,im)\n",
    "    img_true=np.array(img_true).ravel()\n",
    "    img_pred=np.array(img_pred).ravel()\n",
    "    iou = jaccard_score(img_true, img_pred,average='micro')\n",
    "    ious.append(iou)\n",
    "  return np.mean(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "89F8Q6WdQWdk",
    "outputId": "6c5af3fc-d5f4-4fca-a48b-ec6736ab0f9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9369253384755418"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,trainIm,trainAn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "m99FuGyazUwl",
    "outputId": "4f457fe4-f0d2-4323-e80d-a29f16c9301e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.886735947309882"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,testIm,testAn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "id": "MUF9EoYa0FBC",
    "outputId": "316492f1-3d45-431d-bb0d-58ed08324f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(528, 800)\n",
      "(528, 800)\n",
      "422400\n",
      "422400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "img_true =  res(cv2.cvtColor(cv2.imread(testAn[0]), cv2.COLOR_BGR2GRAY),528,800)\n",
    "img_pred = predict(model,testIm[0])\n",
    "print(img_true.shape)\n",
    "print(img_pred.shape)\n",
    "img_true=np.array(img_true).ravel()\n",
    "img_pred=np.array(img_pred).ravel()\n",
    "print(len(img_true))\n",
    "print(len(img_pred))\n",
    "iou = jaccard_similarity_score(img_true, img_pred)\n",
    "\n",
    "iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wSpWJYpeeQOM"
   },
   "outputs": [],
   "source": [
    "out = model.predict_segmentation(\n",
    "    inp=testIm[0],\n",
    "    out_fname=\"1cout.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7h1Hy6aPaxP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "03ttUKUKnEvk",
    "outputId": "aa9cb11e-c8eb-4020-a4ea-6f464cf17706"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 7, 1, 8, 4, 6, 2, 5, 3, 9])"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(out.view().ravel('K')).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3AvSGmNnTfp"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Hj_rGl0F5Ze6",
    "outputId": "24691cb5-07cf-47c5-997c-b4d1416153a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8537831439393939"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "image-segmentation-tests.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
